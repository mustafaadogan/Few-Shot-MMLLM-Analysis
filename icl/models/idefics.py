from .models import model_registry
from tqdm import tqdm
from utils.util import write_results

import torch
from transformers import IdeficsForVisionText2Text, AutoProcessor
import random


def calculate_score(generated_text, caption_order):
    """
    Calculate the score based on generated text and caption order.

    Parameters:
    - generated_text (str): The text generated by the model.
    - caption_order (int): The order of the captions (0 or 1).

    Returns:
    list: A list representing the score, e.g., [1, 0] for correct, [0, 1] for incorrect, [-1, -1] for not found.
    """
    result = [-1, -1]
    correct_caption = "Final Answer: caption 0"
    incorrect_caption = "Final Answer: caption 1"

    if caption_order == 1:
        correct_caption, incorrect_caption = incorrect_caption, correct_caption

    if (correct_caption in generated_text) and (incorrect_caption not in generated_text):
        result = [1, 0]
    elif (correct_caption not in generated_text) and (incorrect_caption in generated_text):
        result = [0, 1]
    else:
        print("calculate_score mismatch!")

    return result


class Idefics:
    def __init__(self):
        self.model = None
        self.processor = None
        self.results = {}
        random.seed(42)

    def load_model(self, device):
        """
        Load the IDEFICS model and processor.

        Parameters:
        - device: The device on which to load the model.

        Returns:
        None
        """
        checkpoint_path = "HuggingFaceM4/idefics-9b-instruct"
        self.device = device
        self.model = IdeficsForVisionText2Text.from_pretrained(checkpoint_path, torch_dtype=torch.bfloat16).to(device)
        self.processor = AutoProcessor.from_pretrained(checkpoint_path)

    def generate_text(self, user_prompt, support_data, query_data):
        """
        Generate text based on the given prompts, support data, and query data.

        Parameters:
        - user_prompt (str): User prompt for the assistant.
        - support_data (list): List of dictionaries containing support data.
        - query_data (dict): Dictionary containing query data.

        Returns:
        dict: Dictionary containing generated text data.
        """
        prompt = []
        for item in support_data:
            img, caption, foil = item['image'], item['caption'], item['foil']
            caption_order = random.randint(0, 1)
            
            if caption_order == 1:
                caption, foil = foil, caption

            prompt += [
                "User:",
                img,
                f"{user_prompt} caption 0: {caption} caption 1: {foil}. \nAssistant:Final Answer: caption {caption_order}\n"
            ]

        query_img, query_caption_text, query_foil_text = query_data['image'], query_data['caption'], query_data['foil']
        query_caption_order = random.randint(0, 1)
        
        if query_caption_order == 1:
            query_caption_text, query_foil_text = query_foil_text, query_caption_text

        prompt += [
            "User:",
            query_img,
            f"{user_prompt} caption 0: {query_caption_text} caption 1: {query_foil_text}. \nAssistant:"
        ]
        
        query_prompt = prompt[-1]

        # --batched mode
        inputs = self.processor(prompt, return_tensors="pt").to(self.device)
        bad_words_ids = self.processor.tokenizer(["<image>", "<fake_token_around_image>"], add_special_tokens=False).input_ids

        generated_ids = self.model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)
        raw_result = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print(f"raw_result = {raw_result}")
        
        salt_result_index = raw_result.find(query_prompt) + len(query_prompt)
        salt_result = raw_result[salt_result_index:]
        
        result = {'raw_result': raw_result,
                  'salt_result': salt_result,
                  'query_caption_order': query_caption_order,
                  'prompt': str(prompt)}

        return result

    def test(self, data, scoring_type):
        """
        Test the IDEFICS model on the given data.

        Parameters:
        - data (list): List of data items.
        - scoring_type: Scoring type (not used in the provided code).

        Returns:
        None
        """
        for item in tqdm(data):
            assert len(item['support_classes_raw_texts']) == len(item['support_classes_foil_raw_texts']), "Caption-Foil count mismatch!"
            assert len(item['support_classes_image_list']) == len(item['support_classes_raw_texts']), "Image-Caption count mismatch!"
            
            support_data = [{'image': img, 'caption': caption, 'foil': foil}
                            for img, (caption, foil) in zip(item['support_classes_image_list'],
                                                            item['support_classes_raw_texts'])]
            query_data = {'image': item['query_image'],
                          'caption': item['query_raw_texts'][0],
                          'foil': item['query_raw_texts'][1]}

            generated_text_info = self.generate_text(item['prompt'], support_data, query_data)
            score = calculate_score(generated_text_info['salt_result'], generated_text_info['query_caption_order'])

            item_id = item['item_id']
            self.results[item_id] = {'scores': score,
                                     'query_caption_order': generated_text_info['query_caption_order'],
                                     'prompt': generated_text_info['prompt'],
                                     'raw_result': generated_text_info['raw_result'],
                                     'salt_result': generated_text_info['salt_result']}

    def prepare_results(self, task_name):
        """
        Prepare and write the results to a JSON file.

        Parameters:
        - task_name (str): Name of the task.

        Returns:
        None
        """
        write_results(task_name[:-5] + "_idefics.json", self.results)


idefics_instance = Idefics()
model_registry.register_model("idefics", (idefics_instance.load_model, idefics_instance.test, idefics_instance.prepare_results))
